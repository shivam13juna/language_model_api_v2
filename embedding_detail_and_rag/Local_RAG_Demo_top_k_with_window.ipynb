{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b374c02",
   "metadata": {},
   "source": [
    "\n",
    "# Local RAG Demo (Jupyter) — Ollama Embeddings + FAISS\n",
    "\n",
    "\n",
    "Try these in a terminal:\n",
    "```bash\n",
    "ollama list\n",
    "ollama pull nomic-embed-text\n",
    "ollama pull gemma:2b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "573eec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If needed, install dependencies. (Safe to re-run.)\n",
    "%pip -q install  faiss-cpu numpy pandas tqdm requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f77d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Configuration (change these as needed) ===\n",
    "EMBED_MODEL = \"embeddinggemma:latest\"   \n",
    "#EMBED_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "WORDS_PER_CHUNK = 300\n",
    "OVERLAP_WORDS   = 60\n",
    "TOPK            = 5\n",
    "\n",
    "# Toggle downloads off if you want to stay strictly offline (then we'll use tiny built-in samples).\n",
    "DOWNLOAD_FROM_WEB = True\n",
    "\n",
    "# A small selection of long classics (public domain). Even 2–3 will give you 300+ chunks.\n",
    "\n",
    "GUTENBERG_BOOKS = {\n",
    "    \"Moby-Dick\": \"https://www.gutenberg.org/files/2701/2701-0.txt\",\n",
    "    \"Pride and Prejudice\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
    "    \"Frankenstein\": \"https://www.gutenberg.org/files/84/84-0.txt\",\n",
    "    \"Alice in Wonderland\": \"https://www.gutenberg.org/cache/epub/11/pg11.txt\",\n",
    "    \"Dracula\": \"https://www.gutenberg.org/files/345/345-0.txt\",\n",
    "    \"A Tale of Two Cities\": \"https://www.gutenberg.org/files/98/98-0.txt\",\n",
    "    \"The Great Gatsby\": \"https://www.gutenberg.org/cache/epub/64317/pg64317.txt\",\n",
    "    \"Adventures of Sherlock Holmes\": \"https://www.gutenberg.org/files/1661/1661-0.txt\",\n",
    "    \"War and Peace\": \"https://www.gutenberg.org/files/2600/2600-0.txt\",\n",
    "    \"Jane Eyre\": \"https://www.gutenberg.org/files/1260/1260-0.txt\",\n",
    "    \"The Picture of Dorian Gray\": \"https://www.gutenberg.org/files/174/174-0.txt\",\n",
    "    \"Crime and Punishment\": \"https://www.gutenberg.org/files/2554/2554-0.txt\",\n",
    "    \"Wuthering Heights\": \"https://www.gutenberg.org/files/768/768-0.txt\",\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "GUTENBERG = [\n",
    "    (title, url) for title, url in GUTENBERG_BOOKS.items()\n",
    "]\n",
    "\n",
    "CORPUS_DIR = \"corpus_jupyter\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfde41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, json, textwrap\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ollama\n",
    "import faiss\n",
    "\n",
    "import truststore\n",
    "truststore.inject_into_ssl()\n",
    "\n",
    "# Minimal helper: strip Project Gutenberg boilerplate if found.\n",
    "START_MARK = re.compile(r\"\\*\\*\\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", re.I)\n",
    "END_MARK   = re.compile(r\"\\*\\*\\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", re.I)\n",
    "\n",
    "def strip_gutenberg_boilerplate(txt: str) -> str:\n",
    "    start = START_MARK.search(txt)\n",
    "    end = END_MARK.search(txt)\n",
    "    if start and end and end.start() > start.end():\n",
    "        return txt[start.end():end.start()].strip()\n",
    "    # Fallback heuristics\n",
    "    txt = re.sub(r\"(?s)^.*?Project Gutenberg.*?eBook.*?\\n\", \"\", txt, flags=re.I)\n",
    "    txt = re.sub(r\"(?s)End of Project Gutenberg.*$\", \"\", txt, flags=re.I)\n",
    "    return txt.strip()\n",
    "\n",
    "def l2_normalize(mat: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True) + 1e-12\n",
    "    return mat / norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "231c5ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found pre-built artifacts! Loading from disk …\n",
      "   8509 chunks | embeddings (8509, 768) | FAISS 8509 vectors\n",
      "   Skipping download, chunking, embedding, and index-build cells.\n"
     ]
    }
   ],
   "source": [
    "# === Check if pre-built artifacts exist; if so, load and skip heavy work ===\n",
    "import json as _json\n",
    "\n",
    "ARTIFACTS_DIR = \"rag_artifacts\"\n",
    "_artifact_files = [\"chunks.json\", \"embeddings.npy\", \"faiss_index.bin\", \"config.json\"]\n",
    "ARTIFACTS_LOADED = all((Path(ARTIFACTS_DIR) / f).exists() for f in _artifact_files)\n",
    "\n",
    "if ARTIFACTS_LOADED:\n",
    "    print(\"✅ Found pre-built artifacts! Loading from disk …\")\n",
    "    with open(Path(ARTIFACTS_DIR) / \"chunks.json\", encoding=\"utf-8\") as _f:\n",
    "        chunks = _json.load(_f)\n",
    "    emb = np.load(str(Path(ARTIFACTS_DIR) / \"embeddings.npy\"))\n",
    "    index = faiss.read_index(str(Path(ARTIFACTS_DIR) / \"faiss_index.bin\"))\n",
    "    print(f\"   {len(chunks)} chunks | embeddings {emb.shape} | FAISS {index.ntotal} vectors\")\n",
    "    print(\"   Skipping download, chunking, embedding, and index-build cells.\")\n",
    "else:\n",
    "    print(\"⏳ Artifacts not found — will build from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd76ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipped (artifacts already loaded)\n"
     ]
    }
   ],
   "source": [
    "if not ARTIFACTS_LOADED:\n",
    "    Path(CORPUS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    docs = []  # list of dicts: {title, text, path}\n",
    "\n",
    "    if DOWNLOAD_FROM_WEB:\n",
    "        for title, url in GUTENBERG:\n",
    "            out_path = Path(CORPUS_DIR) / f\"{title.replace(' ', '_')}.txt\"\n",
    "            if not out_path.exists():\n",
    "                print(f\"Downloading: {title}\")\n",
    "                try:\n",
    "                    r = requests.get(url, timeout=60)\n",
    "                    r.raise_for_status()\n",
    "                    clean = strip_gutenberg_boilerplate(r.text)\n",
    "                    out_path.write_text(clean, encoding=\"utf-8\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed ({e}); skipping.\")\n",
    "            else:\n",
    "                print(f\"Exists: {out_path.name}\")\n",
    "            if out_path.exists():\n",
    "                docs.append({\n",
    "                    \"title\": title,\n",
    "                    \"text\": out_path.read_text(encoding=\"utf-8\", errors=\"ignore\"),\n",
    "                    \"path\": str(out_path)\n",
    "                })\n",
    "\n",
    "    print(f\"Loaded {len(docs)} docs\")\n",
    "else:\n",
    "    print(\"⏩ Skipped (artifacts already loaded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8946fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipped (artifacts already loaded)\n"
     ]
    }
   ],
   "source": [
    "if not ARTIFACTS_LOADED:\n",
    "    # print no of words in each document\n",
    "    for d in docs:\n",
    "        num_words = len(d[\"text\"].split())\n",
    "        print(f\"{d['title']}: {num_words} words\")\n",
    "else:\n",
    "    print(\"⏩ Skipped (artifacts already loaded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dc8f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipped — 8509 chunks already loaded from artifacts\n"
     ]
    }
   ],
   "source": [
    "if not ARTIFACTS_LOADED:\n",
    "    # --- Chunking ---\n",
    "    chunks = []  # list of dicts: {id, title, text, preview, source_path, chunk_index}\n",
    "    for d in docs:\n",
    "        words = d[\"text\"].split()\n",
    "        if not words:\n",
    "            continue\n",
    "        step = max(1, WORDS_PER_CHUNK - OVERLAP_WORDS)\n",
    "        idx = 0\n",
    "        chunk_i = 0\n",
    "        while idx < len(words):\n",
    "            segment = words[idx:idx+WORDS_PER_CHUNK]\n",
    "            if len(segment) < max(60, WORDS_PER_CHUNK//4):\n",
    "                break\n",
    "            text_seg = \" \".join(segment)\n",
    "            chunks.append({\n",
    "                \"id\": f\"{Path(d['title']).name.replace(' ', '_')}#chunk{chunk_i}\",\n",
    "                \"title\": d[\"title\"],\n",
    "                \"text\": text_seg,\n",
    "                \"preview\": text_seg[:400],\n",
    "                \"source_path\": d[\"path\"],\n",
    "                \"chunk_index\": chunk_i,\n",
    "            })\n",
    "            chunk_i += 1\n",
    "            idx += step\n",
    "\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "    pd.DataFrame([\n",
    "        {\"id\": c[\"id\"], \"title\": c[\"title\"], \"preview\": c[\"preview\"][:140] + (\"…\" if len(c[\"preview\"])>140 else \"\")}\n",
    "        for c in chunks[:8]\n",
    "    ])\n",
    "else:\n",
    "    print(f\"⏩ Skipped — {len(chunks)} chunks already loaded from artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a4154be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Embed each chunk with your Ollama embedding model (one-by-one API).\n",
    "## NOTE: Ensure EMBED_MODEL is a valid embeddings model in `ollama list`.\n",
    "#emb_vectors = []\n",
    "#for c in tqdm(chunks, desc=f\"Embedding with {EMBED_MODEL}\"):\n",
    "#    resp = ollama.embeddings(model=EMBED_MODEL, prompt=c[\"text\"])\n",
    "#    v = np.asarray(resp[\"embedding\"], dtype=\"float32\")\n",
    "#    emb_vectors.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbfc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff122b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipped — embeddings already loaded from artifacts\n"
     ]
    }
   ],
   "source": [
    "if not ARTIFACTS_LOADED:\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    import ollama\n",
    "    import faiss\n",
    "\n",
    "    # Tune this based on your machine; 4–8 is usually the sweet spot.\n",
    "    MAX_WORKERS = 6\n",
    "    RETRIES = 3\n",
    "    BACKOFF = 0.6  # seconds, linear backoff\n",
    "\n",
    "    def embed_text(text: str) -> np.ndarray:\n",
    "        last_err = None\n",
    "        for attempt in range(1, RETRIES + 1):\n",
    "            try:\n",
    "                resp = ollama.Client(host=\"http://localhost:11434\", trust_env=False).embeddings(model=EMBED_MODEL, prompt=text)\n",
    "                return np.asarray(resp[\"embedding\"], dtype=\"float32\")\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                if attempt < RETRIES:\n",
    "                    time.sleep(BACKOFF * attempt)\n",
    "        raise last_err\n",
    "\n",
    "    # Parallelize over chunks while preserving original order\n",
    "    emb_vectors = [None] * len(chunks)\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futures = {ex.submit(embed_text, c[\"text\"]): i for i, c in enumerate(chunks)}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"Embedding ({EMBED_MODEL})\"):\n",
    "            i = futures[fut]\n",
    "            emb_vectors[i] = fut.result()\n",
    "else:\n",
    "    print(f\"⏩ Skipped — embeddings already loaded from artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f6a4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipped — FAISS index already loaded (8509 vectors)\n"
     ]
    }
   ],
   "source": [
    "if not ARTIFACTS_LOADED:\n",
    "    emb = np.vstack(emb_vectors)  # shape (N, d)\n",
    "    d = emb.shape[1]\n",
    "    print(\"Embeddings shape:\", emb.shape)\n",
    "\n",
    "    # L2-normalize and build FAISS IP index (IP on unit-norm == cosine similarity)\n",
    "    emb = l2_normalize(emb)\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    index.add(emb)\n",
    "    print(\"FAISS index size:\", index.ntotal)\n",
    "else:\n",
    "    print(f\"⏩ Skipped — FAISS index already loaded ({index.ntotal} vectors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4f40c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Skipped — artifacts already exist on disk\n"
     ]
    }
   ],
   "source": [
    "if not ARTIFACTS_LOADED:\n",
    "    # === Save all time-consuming artifacts to disk for fast Flask deployment ===\n",
    "    import json, pickle\n",
    "\n",
    "    ARTIFACTS_DIR = \"rag_artifacts\"\n",
    "    Path(ARTIFACTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1. Save chunks metadata as JSON\n",
    "    with open(Path(ARTIFACTS_DIR) / \"chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False)\n",
    "    print(f\"Saved {len(chunks)} chunks to {ARTIFACTS_DIR}/chunks.json\")\n",
    "\n",
    "    # 2. Save normalized embeddings as numpy array\n",
    "    np.save(Path(ARTIFACTS_DIR) / \"embeddings.npy\", emb)\n",
    "    print(f\"Saved embeddings shape {emb.shape} to {ARTIFACTS_DIR}/embeddings.npy\")\n",
    "\n",
    "    # 3. Save FAISS index\n",
    "    faiss.write_index(index, str(Path(ARTIFACTS_DIR) / \"faiss_index.bin\"))\n",
    "    print(f\"Saved FAISS index ({index.ntotal} vectors) to {ARTIFACTS_DIR}/faiss_index.bin\")\n",
    "\n",
    "    # 4. Save config so Flask app knows model name, chunk params, etc.\n",
    "    config = {\n",
    "        \"EMBED_MODEL\": EMBED_MODEL,\n",
    "        \"WORDS_PER_CHUNK\": WORDS_PER_CHUNK,\n",
    "        \"OVERLAP_WORDS\": OVERLAP_WORDS,\n",
    "        \"TOPK\": TOPK,\n",
    "    }\n",
    "    with open(Path(ARTIFACTS_DIR) / \"config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"Saved config to {ARTIFACTS_DIR}/config.json\")\n",
    "\n",
    "    print(\"\\n✅ All artifacts saved!\")\n",
    "else:\n",
    "    print(\"⏩ Skipped — artifacts already exist on disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b8e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a83969c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mbreak\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f40789",
   "metadata": {},
   "source": [
    "## Step-by-Step Breakdown: `expand_with_neighbors_simple()`\n",
    "\n",
    "This function takes the top few chunks found by FAISS and expands them by grabbing neighboring chunks, then merges contiguous ones.\n",
    "\n",
    "### Input Parameters:\n",
    "- **`I`**: Indices array of shape `(1, num_hits)` — contains global chunk indices of matched chunks in order of relevance\n",
    "- **`D`**: Distances array of shape `(1, num_hits)` — contains similarity scores for each match\n",
    "- **`chunks`**: List of all chunk dictionaries with metadata\n",
    "- **`neighbors`**: How many chunks before/after each hit to grab (e.g., 1 = grab ±1, so [-1, 0, +1])\n",
    "- **`max_out`**: Maximum total chunks to return (safety limit)\n",
    "\n",
    "### Step 1: Initialize Tracking Structures\n",
    "```python\n",
    "seen = set()            # Track which chunks we've already included (avoid duplicates)\n",
    "ordered_idxs = []       # Maintain order of chunks as we add them\n",
    "```\n",
    "\n",
    "### Step 2: Walk Through Hits in Relevance Order\n",
    "For each hit from FAISS (best match first):\n",
    "1. Extract the chunk metadata and its document path + chunk index\n",
    "2. For each neighbor offset (-neighbors to +neighbors):\n",
    "   - Try to find that neighbor in the `KEY_TO_IDX` lookup map\n",
    "   - If it exists AND we haven't seen it: add it to our list\n",
    "   - If we've hit max chunks: stop immediately\n",
    "\n",
    "**Example:** If hit is chunk 5 and neighbors=1:\n",
    "- Check chunks 4, 5, 6\n",
    "- If they exist and we haven't grabbed them, add all three\n",
    "\n",
    "### Step 3: Sort by Document and Chunk Order\n",
    "```python\n",
    "ordered_idxs.sort(key=lambda j: (chunks[j][\"source_path\"], chunks[j][\"chunk_index\"]))\n",
    "```\n",
    "Why? Because we added chunks in relevance order, but now we need them in document order to detect contiguous ranges.\n",
    "\n",
    "**Example Result:** \n",
    "- Before: [5, 4, 12, 6]  (relevance order from FAISS)\n",
    "- After: [4, 5, 6, 12]  (document/chunk order)\n",
    "\n",
    "### Step 4: Merge Contiguous Chunks into Spans\n",
    "Walk through sorted chunks and group consecutive ones:\n",
    "\n",
    "1. **Start a span** with the first chunk\n",
    "2. **For each next chunk:**\n",
    "   - If it's from the **same document** AND its chunk_index is **exactly one more** than the current span's end:\n",
    "     - **Extend** the existing span (increment end_ci, add chunk to list)\n",
    "   - Otherwise:\n",
    "     - **Save** the previous span\n",
    "     - **Start a new span** with this chunk\n",
    "3. **Don't forget** the last span\n",
    "\n",
    "**Example:**\n",
    "- Input chunks (sorted): 4, 5, 6 (same doc, chunk_index 4→5→6)\n",
    "- Output span: `{start_ci: 4, end_ci: 6, idxs: [chunk4, chunk5, chunk6]}`\n",
    "\n",
    "### Step 5: Build Final Context Objects\n",
    "For each merged span:\n",
    "1. **Join texts** of all chunks in the span with \"\\n\\n\"\n",
    "2. **Create metadata**: title, source_path, chunk range, word count, span size\n",
    "3. **Return** list of context objects ready for the LLM\n",
    "\n",
    "**Output Example:**\n",
    "```python\n",
    "{\n",
    "    \"title\": \"Pride and Prejudice\",\n",
    "    \"source_path\": \"/path/to/file\",\n",
    "    \"start_chunk\": 4,\n",
    "    \"end_chunk\": 6,\n",
    "    \"text\": \"[full merged text]\",\n",
    "    \"approx_words\": 900,  # sum of all chunks\n",
    "    \"span_count\": 3\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850721f",
   "metadata": {},
   "source": [
    "## Step-by-Step Breakdown: `search_windowed()`\n",
    "\n",
    "This is the main search orchestrator. It takes a user query and returns both raw FAISS hits and intelligently expanded contexts.\n",
    "\n",
    "### Input Parameters:\n",
    "- **`query`**: User's natural language question/search string\n",
    "- **`topk`**: Number of initial FAISS hits to retrieve (default: TOPK=5)\n",
    "- **`max_out`**: Maximum chunks in final output (default: 8)\n",
    "\n",
    "### Step 1: Embed the Query\n",
    "```python\n",
    "q_vec = ollama.Client().embeddings(model=EMBED_MODEL, prompt=query)\n",
    "q_vec = q_vec / (np.linalg.norm(q_vec) + 1e-12)  # L2 normalize\n",
    "```\n",
    "\n",
    "**What's happening:**\n",
    "- Convert the query text into a numerical vector using the Ollama embedding model\n",
    "- Normalize it so its length = 1 (this makes cosine similarity = dot product in FAISS)\n",
    "- The `+1e-12` prevents division by zero for edge cases\n",
    "\n",
    "**Output:** A 384-dimensional vector representing the query's meaning\n",
    "\n",
    "### Step 2: Search FAISS for Top-K Matches\n",
    "```python\n",
    "D, I = index.search(q_vec.reshape(1, -1), topk)\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- FAISS searches for the `topk` most similar chunks to your query\n",
    "- Returns:\n",
    "  - **`I[0]`**: Indices of top-k chunks (e.g., [5, 12, 3, 8, 1])\n",
    "  - **`D[0]`**: Similarity scores (e.g., [0.89, 0.76, 0.65, 0.58, 0.52])\n",
    "\n",
    "**Example interpretation:**\n",
    "- Chunk #5 is most similar (score 0.89)\n",
    "- Chunk #12 is second (score 0.76)\n",
    "- etc., ordered by relevance\n",
    "\n",
    "### Step 3: Decide Neighbor Window Size (Heuristic)\n",
    "```python\n",
    "neighbors = 1 if WORDS_PER_CHUNK >= 260 else 2\n",
    "```\n",
    "\n",
    "**Logic:**\n",
    "- **Large chunks (≥260 words):** Each chunk already has lots of context, so grab only ±1 neighbors\n",
    "- **Small chunks (<260 words):** Need more context, so grab ±2 neighbors\n",
    "\n",
    "**Why?** Prevents context gaps while keeping output reasonable.\n",
    "\n",
    "### Step 4: Decide How Many Hits to Expand (Heuristic)\n",
    "```python\n",
    "top1 = float(D[0][0])           # Best match score\n",
    "top2 = float(D[0][1]) if len(D[0]) > 1 else 0.0\n",
    "margin = top1 - top2            # Gap between 1st and 2nd\n",
    "\n",
    "init_topk = 1 if (top1 >= 0.35 and margin >= 0.05) else min(3, topk)\n",
    "```\n",
    "\n",
    "**Logic:**\n",
    "- If **top match is confident** (score ≥ 0.35) AND **clearly wins** (margin ≥ 0.05):\n",
    "  - Use only `1` hit (the best one is enough)\n",
    "- Otherwise:\n",
    "  - Use up to `3` hits (cast a wider net)\n",
    "\n",
    "**Intuition:**\n",
    "- Query: \"Who is Sherlock Holmes?\" → top hit: Sherlock bio (0.89) vs. second (0.70) = margin 0.19 → Use 1 hit\n",
    "- Query: \"Victorian clothing\" → top hit: costume desc (0.75) vs. second (0.72) = margin 0.03 → Use 3 hits (competing sources)\n",
    "\n",
    "### Step 5: Slice to Get Only Selected Hits\n",
    "```python\n",
    "D_init = np.array([D[0][:init_topk]])\n",
    "I_init = np.array([I[0][:init_topk]])\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- If `init_topk=1`: Take only the top 1 hit's index and score\n",
    "- If `init_topk=3`: Take the top 3 hits\n",
    "\n",
    "**Result:** Focused arrays for neighbor expansion\n",
    "\n",
    "### Step 6: Expand with Neighbors and Merge\n",
    "```python\n",
    "contexts = expand_with_neighbors_simple(I_init, D_init, chunks, \n",
    "                                        neighbors=neighbors, max_out=max_out)\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Call the function we explained above\n",
    "- Returns: List of merged context objects ready for the LLM\n",
    "\n",
    "**Example output:**\n",
    "```python\n",
    "[\n",
    "    {\n",
    "        \"title\": \"Adventures of Sherlock Holmes\",\n",
    "        \"source_path\": \"...\",\n",
    "        \"start_chunk\": 5,\n",
    "        \"end_chunk\": 7,\n",
    "        \"text\": \"[1200 words of merged chunks 5, 6, 7]\",\n",
    "        \"approx_words\": 1200,\n",
    "        \"span_count\": 3\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"A Scandal in Bohemia\",  # Different doc, gap in indexes\n",
    "        \"source_path\": \"...\",\n",
    "        \"start_chunk\": 120,\n",
    "        \"end_chunk\": 120,\n",
    "        \"text\": \"[350 words]\",\n",
    "        \"approx_words\": 350,\n",
    "        \"span_count\": 1\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "### Step 7: Build Display DataFrame (All K Hits)\n",
    "```python\n",
    "rows = []\n",
    "for score, idx_ in zip(D[0].tolist(), I[0].tolist()):\n",
    "    m = chunks[idx_]\n",
    "    rows.append({\n",
    "        \"score\": round(score, 3),\n",
    "        \"id\": m[\"id\"],\n",
    "        \"title\": m[\"title\"],\n",
    "        \"source_path\": m[\"source_path\"],\n",
    "        \"chunk_index\": m[\"chunk_index\"],\n",
    "        \"preview\": (m[\"preview\"][:220] + \"…\") if len(m[\"preview\"])>220 else m[\"preview\"],\n",
    "    })\n",
    "df_hits = pd.DataFrame(rows)\n",
    "```\n",
    "\n",
    "**Purpose:** Show all `topk` FAISS hits for transparency/debugging\n",
    "\n",
    "**Example output table:**\n",
    "\n",
    "| score | id              | title          | chunk_index | preview         |\n",
    "|-------|-----------------|----------------|-------------|-----------------|\n",
    "| 0.891 | Holmes#chunk4   | Sherlock...    | 4           | \"The adventure began when...\"  |\n",
    "| 0.756 | Holmes#chunk12  | Sherlock...    | 12          | \"Watson exclaimed in...\"        |\n",
    "| 0.654 | Holmes#chunk18  | Sherlock...    | 18          | \"The criminal mastermind...\"   |\n",
    "\n",
    "### Step 8: Return Results\n",
    "```python\n",
    "return df_hits, contexts\n",
    "```\n",
    "\n",
    "**Returns:**\n",
    "1. **`df_hits`**: DataFrame showing all raw FAISS hits (for debugging)\n",
    "2. **`contexts`**: List of merged context objects (for LLM input)\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Example Flow\n",
    "\n",
    "**Query:** `\"Elizabeth's love for Darcy\"`\n",
    "\n",
    "1. ✅ Embed query → vector [0.341, -0.129, ..., 0.554]\n",
    "2. ✅ FAISS search → I=[42, 108, 33, 5, 71], D=[0.87, 0.65, 0.58, 0.52, 0.49]\n",
    "3. ✅ Decision: score=0.87 ≥ 0.35? YES. margin=(0.87-0.65)=0.22 ≥ 0.05? YES. → Use 1 hit\n",
    "4. ✅ Expand chunk 42 with neighbors (±1) → get chunks [41, 42, 43]\n",
    "5. ✅ Merge: All same doc, chunks 41→42→43 are contiguous → span {start:41, end:43}\n",
    "6. ✅ Return: df_hits with all 5 raw hits, contexts with 1 merged span (3 chunks joined)\n",
    "\n",
    "**Output to LLM:** ~900 words of continuous context about Elizabeth & Darcy's relationship\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce0f35",
   "metadata": {},
   "source": [
    "## Visual System Flow\n",
    "\n",
    "### High-Level Architecture Diagram\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                     USER QUERY                                      │\n",
    "│              \"Elizabeth's love for Darcy\"                          │\n",
    "└────────────────────────────┬────────────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                  ┌──────────────────────┐\n",
    "                  │  Embed Query         │\n",
    "                  │  (Ollama model)      │\n",
    "                  │  384-dim vector      │\n",
    "                  └──────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                  ┌──────────────────────────────────────┐\n",
    "                  │  FAISS ANN Search (topk=5)           │\n",
    "                  │                                      │\n",
    "                  │  Returns:                            │\n",
    "                  │  I = [42, 108, 33, 5, 71]  (indices)│\n",
    "                  │  D = [0.87, 0.65, 0.58...] (scores) │\n",
    "                  └──────────────────────────────────────┘\n",
    "                             │\n",
    "                  ┌──────────┴──────────┐\n",
    "                  │                     │\n",
    "        Save (for debugging)   Decide Expansion Strategy\n",
    "                  │                     │\n",
    "                  │                ┌────▼─────────┐\n",
    "                  │                │ Is top match │\n",
    "                  │                │  strong &    │\n",
    "                  │                │  confident?  │\n",
    "                  │                └────┬─────────┘\n",
    "                  │                     │\n",
    "                  │            ┌────────┴────────┐\n",
    "                  │            │                 │\n",
    "                  │         YES│                 │NO\n",
    "                  │            │                 │\n",
    "                  │       1 hit │          1-3 hits\n",
    "                  │            │                 │\n",
    "                  │            └────────┬────────┘\n",
    "                  │                     │\n",
    "                  │            ┌────────▼──────────┐\n",
    "                  │            │  Expand Neighbors │\n",
    "                  │            │  (±1 or ±2 chunks)│\n",
    "                  │            └─────────┬────────┘\n",
    "                  │                      │\n",
    "                  │            ┌─────────▼────────┐\n",
    "                  │            │ Merge Contiguous │\n",
    "                  │            │ Chunks into Spans│\n",
    "                  │            └────────┬────────┘\n",
    "                  │                     │\n",
    "        ┌─────────▼──────────┐  ┌──────▼──────────────┐\n",
    "        │   df_hits table    │  │  Expanded Contexts  │\n",
    "        │ (all 5 raw hits)   │  │  (merged spans)     │\n",
    "        └────────────────────┘  └──────┬──────────────┘\n",
    "                                       │\n",
    "                        ┌──────────────▼──────────────┐\n",
    "                        │  Ready for LLM / RAG System │\n",
    "                        │  (Better context window)    │\n",
    "                        └─────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Key Design Decisions Explained\n",
    "\n",
    "### 1. Why Use Two Different Functions?\n",
    "\n",
    "**`expand_with_neighbors_simple()`** = Context Expansion\n",
    "- Knows how to walk FAISS results and grab neighbors\n",
    "- Knows how to merge contiguous chunks\n",
    "- Pure chunk manipulation\n",
    "\n",
    "**`search_windowed()`** = Intelligence Layer\n",
    "- Decides *how many* hits to expand (smart heuristics)\n",
    "- Decides *how wide* the neighbor window should be\n",
    "- Coordinates the overall search strategy\n",
    "\n",
    "**Benefit:** Separation of concerns. Easy to swap in different expansion strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Search FAISS for Topk=5 but Only Expand 1-3?\n",
    "\n",
    "**Problem:** More hits = more API calls, higher latency, more context to process\n",
    "\n",
    "**Solution:** Confidence heuristics\n",
    "- If the top hit is **very confident**, use only that one\n",
    "- If results are **ambiguous** (close scores), use multiple hits for coverage\n",
    "\n",
    "**Example:**\n",
    "- Query: \"Who wrote Moby Dick?\" → Search shows ~90% certainty = 1 hit enough\n",
    "- Query: \"Whaling techniques\" → Multiple chapters discuss it (~similar scores) = 3 hits better\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why Merge Contiguous Chunks?\n",
    "\n",
    "**Problem:** If you expand hits, you get overlapping chunks\n",
    "\n",
    "**Example:**\n",
    "- FAISS hit: chunk 5\n",
    "- Expand ±1: chunks 4, 5, 6\n",
    "- If next hit is chunk 6, you'd have: [4, 5, 6] + [5, 6, 7] = duplication!\n",
    "\n",
    "**Solution:** After gathering all neighbors, merge contiguous ones\n",
    "- Input: [4, 5, 6, 7] → Output: single span 4-7 with merged text\n",
    "- No duplication, no gaps, clean output\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why KEY_TO_IDX Lookup Map?\n",
    "\n",
    "```python\n",
    "KEY_TO_IDX = {(source_path, chunk_index): global_idx ...}\n",
    "```\n",
    "\n",
    "**Problem:** Given a chunk's (document, chunk_number), how do you find its global index?\n",
    "\n",
    "**Before:** Scan through all 300+ chunks every time\n",
    "**After:** O(1) dictionary lookup\n",
    "\n",
    "**Critical for performance** when expanding thousands of queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree: Smart Expansion\n",
    "\n",
    "```\n",
    "Query comes in\n",
    "    │\n",
    "    ├─ Best match score = 0.89, Second = 0.70 (margin=0.19)\n",
    "    │  → Strong & confident\n",
    "    │  → expand_topk = 1\n",
    "    │  → Reasoning: We're very sure of the answer\n",
    "    │\n",
    "    ├─ Best match score = 0.75, Second = 0.71 (margin=0.04)\n",
    "    │  → Weak confidence signal\n",
    "    │  → expand_topk = 3\n",
    "    │  → Reasoning: Ambiguous, need alternative perspectives\n",
    "    │\n",
    "    └─ Chunk size very large (400+ words)\n",
    "       → neighbors = 1 (less context needed)\n",
    "       OR\n",
    "       Chunk size small (150 words)\n",
    "       → neighbors = 2 (more context helpful)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Common Failure Modes & How This Avoids Them\n",
    "\n",
    "| Problem | How This Design Helps |\n",
    "|---------|----------------------|\n",
    "| **Too much context** → LLM confusion | Smart heuristic limits to 1-3 base hits |\n",
    "| **Chunk boundaries cut sentences** | Neighbor expansion gives full context |\n",
    "| **Duplicate chunks in output** | Merging deduplicates seamlessly |\n",
    "| **Slow retrieval** | topk=5 before filtering (not 100) |\n",
    "| **Missing context** | Neighbors handle cross-chunk dependencies |\n",
    "| **Document gaps** | Multiple sources if confidence is low |\n",
    "\n",
    "---\n",
    "\n",
    "## Example Walkthrough: Complete Query Flow\n",
    "\n",
    "```\n",
    "INPUT: query=\"What is the white whale in Moby Dick?\"\n",
    "\n",
    "STEP 1: Embed\n",
    "  → Vector: [0.23, -0.41, ..., 0.67] (384 dims)\n",
    "\n",
    "STEP 2: FAISS Search topk=5\n",
    "  → I = [17, 45, 12, 88, 3]\n",
    "  → D = [0.91, 0.58, 0.52, 0.49, 0.44]\n",
    "\n",
    "STEP 3: Confidence Check\n",
    "  → top1 = 0.91 ≥ 0.35? YES\n",
    "  → margin = 0.91 - 0.58 = 0.33 ≥ 0.05? YES\n",
    "  → init_topk = 1 (confident)\n",
    "\n",
    "STEP 4: Window Size\n",
    "  → WORDS_PER_CHUNK = 300 ≥ 260? YES\n",
    "  → neighbors = 1\n",
    "\n",
    "STEP 5: Expand Hit #17\n",
    "  → Neighbors: chunk 16, 17, 18 (all exist, same doc)\n",
    "  → All added (ordered_idxs = [16, 17, 18])\n",
    "  → Hit max_out? NO, only 3 chunks so far\n",
    "\n",
    "STEP 6: Merge Contiguous\n",
    "  → ordered_idxs sorted: [16, 17, 18]\n",
    "  → All contiguous (16→17→18)\n",
    "  → Single span: {start_ci: 16, end_ci: 18, text: [merged]}\n",
    "\n",
    "OUTPUT:\n",
    "  df_hits: Table of all 5 raw matches (for transparency)\n",
    "  contexts: [\n",
    "    {\n",
    "      title: \"Moby-Dick\",\n",
    "      start_chunk: 16,\n",
    "      end_chunk: 18,\n",
    "      text: \"...Chapter 41: The White Whale...(merged text of 900 words)...\",\n",
    "      approx_words: 900,\n",
    "      span_count: 3\n",
    "    }\n",
    "  ]\n",
    "\n",
    "→ LLM receives one cohesive 900-word context block\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e298cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simplest windowed retrieval: no decay, preserve base-hit order ---\n",
    "\n",
    "\n",
    "# Lookup map for (doc, chunk_index) -> global idx\n",
    "KEY_TO_IDX = {(c[\"source_path\"], c[\"chunk_index\"]): gi for gi, c in enumerate(chunks)}\n",
    "\n",
    "\n",
    "def expand_with_neighbors_simple(I, D, chunks, neighbors=1, max_out=8):\n",
    "    \"\"\"\n",
    "    For the initial ANN hits (I, D), walk hits in rank order and\n",
    "    append each hit plus its ±neighbors (within the same document).\n",
    "    No scoring/decay; just preserve base-hit order and dedupe.\n",
    "    Then merge contiguous chunks into spans and return merged contexts.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    ordered_idxs = []\n",
    "\n",
    "    # Walk initial hits in order\n",
    "    for gi in I[0]:\n",
    "        c = chunks[int(gi)]\n",
    "        doc = c[\"source_path\"]\n",
    "        ci  = c[\"chunk_index\"]\n",
    "        # hit and its neighbors\n",
    "        for delta in range(-neighbors, neighbors + 1):\n",
    "            key = (doc, ci + delta)\n",
    "            j = KEY_TO_IDX.get(key)\n",
    "            if j is None:\n",
    "                continue\n",
    "            if j not in seen:\n",
    "                ordered_idxs.append(j)\n",
    "                seen.add(j)\n",
    "            if len(ordered_idxs) >= max_out:\n",
    "                break\n",
    "        if len(ordered_idxs) >= max_out:\n",
    "            break\n",
    "\n",
    "    # Sort by (doc, chunk_index) for merging\n",
    "    ordered_idxs.sort(key=lambda j: (chunks[j][\"source_path\"], chunks[j][\"chunk_index\"]))\n",
    "\n",
    "    # Merge contiguous chunks from the same doc into spans\n",
    "    merged = []\n",
    "    span = None\n",
    "    for gi in ordered_idxs:\n",
    "        c = chunks[gi]\n",
    "        if (span \n",
    "            and c[\"source_path\"] == span[\"source_path\"] \n",
    "            and c[\"chunk_index\"] == span[\"end_ci\"] + 1):\n",
    "            span[\"end_ci\"] += 1\n",
    "            span[\"idxs\"].append(gi)\n",
    "        else:\n",
    "            if span: merged.append(span)\n",
    "            span = {\n",
    "                \"title\": c[\"title\"],\n",
    "                \"source_path\": c[\"source_path\"],\n",
    "                \"start_ci\": c[\"chunk_index\"],\n",
    "                \"end_ci\": c[\"chunk_index\"],\n",
    "                \"idxs\": [gi],\n",
    "            }\n",
    "    if span: merged.append(span)\n",
    "\n",
    "    # Final contexts\n",
    "    contexts = []\n",
    "    for s in merged:\n",
    "        text = \"\\n\\n\".join(chunks[i][\"text\"] for i in s[\"idxs\"])\n",
    "        contexts.append({\n",
    "            \"title\": s[\"title\"],\n",
    "            \"source_path\": s[\"source_path\"],\n",
    "            \"start_chunk\": s[\"start_ci\"],\n",
    "            \"end_chunk\": s[\"end_ci\"],\n",
    "            \"text\": text,\n",
    "            \"approx_words\": sum(len(chunks[i][\"text\"].split()) for i in s[\"idxs\"]),\n",
    "            \"span_count\": len(s[\"idxs\"]),\n",
    "        })\n",
    "    return contexts\n",
    "\n",
    "\n",
    "def search_windowed(query: str, topk: int = TOPK, max_out: int = 8):\n",
    "    \"\"\"\n",
    "    1) ANN search (topk)\n",
    "    2) Choose neighbor width (based on chunk size)\n",
    "    3) Expand with simple ±neighbors (no decay) and merge\n",
    "    Returns: (df_hits, contexts)\n",
    "    \"\"\"\n",
    "    q_vec = np.asarray(ollama.Client(host=\"http://localhost:11434\", trust_env=False).embeddings(model=EMBED_MODEL, prompt=query)[\"embedding\"], dtype=\"float32\")\n",
    "    q_vec = q_vec / (np.linalg.norm(q_vec) + 1e-12)\n",
    "    D, I = index.search(q_vec.reshape(1, -1), topk)\n",
    "\n",
    "    # Heuristic: with ~300-word chunks, ±1 is usually enough; go ±2 if you shrink chunks later.\n",
    "    neighbors = 1 if WORDS_PER_CHUNK >= 260 else 2\n",
    "\n",
    "    # Margin heuristic to decide how many *base* hits to expand\n",
    "    top1 = float(D[0][0])\n",
    "    top2 = float(D[0][1]) if len(D[0]) > 1 else 0.0\n",
    "    margin = top1 - top2\n",
    "    init_topk = 1 if (top1 >= 0.35 and margin >= 0.05) else min(3, topk)\n",
    "\n",
    "    D_init = np.array([D[0][:init_topk]])\n",
    "    I_init = np.array([I[0][:init_topk]])\n",
    "\n",
    "    contexts = expand_with_neighbors_simple(I_init, D_init, chunks, neighbors=neighbors, max_out=max_out)\n",
    "\n",
    "    # Visible table of ANN hits (unchanged)\n",
    "    rows = []\n",
    "    for score, idx_ in zip(D[0].tolist(), I[0].tolist()):\n",
    "        m = chunks[idx_]\n",
    "        rows.append({\n",
    "            \"score\": round(score, 3),\n",
    "            \"id\": m[\"id\"],\n",
    "            \"title\": m[\"title\"],\n",
    "            \"source_path\": m[\"source_path\"],\n",
    "            \"chunk_index\": m[\"chunk_index\"],\n",
    "            \"preview\": (m[\"preview\"][:220] + \"…\") if len(m[\"preview\"])>220 else m[\"preview\"],\n",
    "        })\n",
    "    df_hits = pd.DataFrame(rows)\n",
    "    return df_hits, contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc0c8ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial ANN Hits ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>source_path</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.517</td>\n",
       "      <td>Pride_and_Prejudice#chunk457</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>corpus_jupyter/Pride_and_Prejudice.txt</td>\n",
       "      <td>457</td>\n",
       "      <td>Mr. Darcy!--and so it does, I vow. Well, any f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.509</td>\n",
       "      <td>Pride_and_Prejudice#chunk83</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>corpus_jupyter/Pride_and_Prejudice.txt</td>\n",
       "      <td>83</td>\n",
       "      <td>employed, Elizabeth could not help observing, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.490</td>\n",
       "      <td>Pride_and_Prejudice#chunk518</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>corpus_jupyter/Pride_and_Prejudice.txt</td>\n",
       "      <td>518</td>\n",
       "      <td>good as a lord! And a special licence--you mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.486</td>\n",
       "      <td>Pride_and_Prejudice#chunk349</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>corpus_jupyter/Pride_and_Prejudice.txt</td>\n",
       "      <td>349</td>\n",
       "      <td>they are! He takes them now for people of fash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.480</td>\n",
       "      <td>Pride_and_Prejudice#chunk357</td>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>corpus_jupyter/Pride_and_Prejudice.txt</td>\n",
       "      <td>357</td>\n",
       "      <td>who had expected to find in her as acute and u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                            id                title  \\\n",
       "0  0.517  Pride_and_Prejudice#chunk457  Pride and Prejudice   \n",
       "1  0.509   Pride_and_Prejudice#chunk83  Pride and Prejudice   \n",
       "2  0.490  Pride_and_Prejudice#chunk518  Pride and Prejudice   \n",
       "3  0.486  Pride_and_Prejudice#chunk349  Pride and Prejudice   \n",
       "4  0.480  Pride_and_Prejudice#chunk357  Pride and Prejudice   \n",
       "\n",
       "                              source_path  chunk_index  \\\n",
       "0  corpus_jupyter/Pride_and_Prejudice.txt          457   \n",
       "1  corpus_jupyter/Pride_and_Prejudice.txt           83   \n",
       "2  corpus_jupyter/Pride_and_Prejudice.txt          518   \n",
       "3  corpus_jupyter/Pride_and_Prejudice.txt          349   \n",
       "4  corpus_jupyter/Pride_and_Prejudice.txt          357   \n",
       "\n",
       "                                             preview  \n",
       "0  Mr. Darcy!--and so it does, I vow. Well, any f...  \n",
       "1  employed, Elizabeth could not help observing, ...  \n",
       "2  good as a lord! And a special licence--you mus...  \n",
       "3  they are! He takes them now for people of fash...  \n",
       "4  who had expected to find in her as acute and u...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Windowed Contexts (merged) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>span</th>\n",
       "      <th>approx_words</th>\n",
       "      <th>span_count</th>\n",
       "      <th>preview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Pride_and_Prejudice.txt</td>\n",
       "      <td>82–84</td>\n",
       "      <td>900</td>\n",
       "      <td>3</td>\n",
       "      <td>Darcy were not such a great tall fellow, in co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Pride_and_Prejudice.txt</td>\n",
       "      <td>456–458</td>\n",
       "      <td>900</td>\n",
       "      <td>3</td>\n",
       "      <td>but she does not know, no one can know, how mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pride and Prejudice</td>\n",
       "      <td>Pride_and_Prejudice.txt</td>\n",
       "      <td>517–518</td>\n",
       "      <td>600</td>\n",
       "      <td>2</td>\n",
       "      <td>utter a syllable. Nor was it under many, many ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title                   source     span  approx_words  \\\n",
       "0  Pride and Prejudice  Pride_and_Prejudice.txt    82–84           900   \n",
       "1  Pride and Prejudice  Pride_and_Prejudice.txt  456–458           900   \n",
       "2  Pride and Prejudice  Pride_and_Prejudice.txt  517–518           600   \n",
       "\n",
       "   span_count                                            preview  \n",
       "0           3  Darcy were not such a great tall fellow, in co...  \n",
       "1           3  but she does not know, no one can know, how mu...  \n",
       "2           2  utter a syllable. Nor was it under many, many ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# --- Quick test query (non-interactive) ---\n",
    "QUERY = \"Elizabeth Bennet's changing feelings for Mr. Darcy\"\n",
    "df_hits, contexts = search_windowed(QUERY, topk=TOPK, max_out=8)\n",
    "\n",
    "print(\"=== Initial ANN Hits ===\")\n",
    "display(df_hits)\n",
    "\n",
    "print(\"\\n=== Windowed Contexts (merged) ===\")\n",
    "display(pd.DataFrame([{\n",
    "    \"title\": c[\"title\"],\n",
    "    \"source\": Path(c[\"source_path\"]).name,\n",
    "    \"span\": f\"{c['start_chunk']}–{c['end_chunk']}\",\n",
    "    \"approx_words\": c[\"approx_words\"],\n",
    "    \"span_count\": c[\"span_count\"],\n",
    "    \"preview\": (c[\"text\"][:220] + \"…\") if len(c[\"text\"]) > 220 else c[\"text\"]\n",
    "} for c in contexts]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
