{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e40c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "#This is optional. I use VPN in my computer. Why I need this. \n",
    "import truststore\n",
    "truststore.inject_into_ssl()\n",
    "\n",
    "\n",
    "\n",
    "def pretty_print(*args):\n",
    "    text = \" \".join(str(arg) for arg in args)\n",
    "    try:\n",
    "        print(textwrap.fill(text, width=80))\n",
    "    except Exception as e:\n",
    "        print(text)  # fallback to normal print if text is not a string\n",
    "\n",
    "        \n",
    "\n",
    "load_dotenv('/Users/shivam13juna/Documents/scaler/iitr_classes/jan_2026/language_model_api_v2/openai_key.env')  # reads .env file in the current directory\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"OPENAI_API_KEY not found! \"\n",
    "        \"Make sure you have a .env file with: OPENAI_API_KEY=sk-...\"\n",
    "    )\n",
    "\n",
    "pretty_print(\"API key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9a5e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client ready.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "pretty_print(\"OpenAI client ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9117e",
   "metadata": {},
   "source": [
    "# Responses API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b979fe9f",
   "metadata": {},
   "source": [
    "## Diff b/w Chat Completions and Responses API - Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eec181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat Completions output: A list comprehension is a concise way to create a new\n",
      "list by transforming items from an existing iterable, optionally filtering them.\n",
      "Basic syntax: - [expression for item in iterable] - You can add an optional\n",
      "condition: [expression for item in iterable if condition]  Examples: - Squares\n",
      "of numbers 0 through 9: squares = [x*x for x in range(10)] - Even numbers from 0\n",
      "to 19: evens = [n for n in range(20) if n % 2 == 0] - Uppercase words: upper =\n",
      "[s.upper() for s in [\"apple\",\"banana\"]] - Flattening a list of lists: flat = [c\n",
      "for sub in [[1,2],[3,4]] for c in sub]  Compared to a loop: - Traditional:\n",
      "squares = []   for x in range(10):       squares.append(x*x) - List\n",
      "comprehension does the same in one line.  Notes: - They’re great for simple\n",
      "transformations and filters, but can hurt readability if too complex. - They\n",
      "create a list in memory. If you want lazy evaluation, use a generator\n",
      "expression: (expression for item in iterable if condition) - You can also use\n",
      "dict/set comprehensions for other collection types.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# do the same with chat completions\n",
    "resp = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    messages=[\n",
    "\t\t{\"role\": \"system\", \"content\": \"You are a friendly Python tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is a list comprehension?\"}\n",
    "    ]\n",
    ")\n",
    "pretty_print(\"Chat Completions output:\", resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36bfb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API output: A list comprehension is a compact way to create a new list\n",
      "by applying an expression to each item in an iterable, optionally filtering\n",
      "items or nesting loops.  Common form: - Basic: [expression for item in iterable]\n",
      "- With a filter: [expression for item in iterable if condition] - With multiple\n",
      "loops: [expression for a in A for b in B]  Examples: - Squares: [x*x for x in\n",
      "range(10)] - Evens from 0–19: [x for x in range(20) if x % 2 == 0] - Pairs: [(i,\n",
      "j) for i in range(3) for j in range(2)]  Notes: - It’s equivalent to building a\n",
      "list with a for-loop, but shorter and often more readable. - If you don’t want\n",
      "to build a list in memory, you can use a generator expression: (expression for\n",
      "item in iterable).\n"
     ]
    }
   ],
   "source": [
    "resp = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"You are a friendly Python tutor.\",\n",
    "    input=\"What is a list comprehension?\"\n",
    ")\n",
    "\n",
    "pretty_print(\"Responses API output:\", resp.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a877c891",
   "metadata": {},
   "source": [
    "## Passing Multi Turn Conversation History to Responses API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f6b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API output: Subjectively, many people point to these as not jerks:  -\n",
      "Keanu Reeves — widely described as gracious, humble, and generous. - Tom Hanks —\n",
      "known for warmth and decency in public appearances. - Dwayne “The Rock” Johnson\n",
      "— often praised for positivity and philanthropy.  Of course, public personas\n",
      "vary by viewer, but these are commonly viewed as pleasant, not jerky, figures.\n",
      "If you want someone from a specific field (music, sports, etc.), I can tailor\n",
      "suggestions.\n"
     ]
    }
   ],
   "source": [
    "input_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is Trump a jerk?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Some people are born that way.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Name a celebrity who is not a jerk.\"}\n",
    "]\n",
    "\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"You are a very candid Journalist.\",\n",
    "    input=input_messages\n",
    ")\n",
    "\n",
    "pretty_print(\"Responses API output:\", resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d3de4",
   "metadata": {},
   "source": [
    "| Feature             | Chat Completions API                             | Responses API                                              |\n",
    "| ------------------- | ------------------------------------------------ | ---------------------------------------------------------- |\n",
    "| **Endpoint**        | `client.chat.completions.create()`               | `client.responses.create()`                                |\n",
    "| **Input format**    | `messages=[{\"role\": ..., \"content\": ...}]`       | `input=` (string or list of message dicts)                 |\n",
    "| **System prompt**   | `{\"role\": \"system\", \"content\": ...}` in messages | `instructions=` parameter (top-level)                      |\n",
    "| **Output access**   | `resp.choices[0].message.content`                | `resp.output_text`                                         |\n",
    "| **Multi-turn**      | Manually pass full message history each time     | `previous_response_id=resp.id` (server-side context)       |\n",
    "| **Developer role**  | Not supported (use `system`)                     | `{\"role\": \"developer\"}` for meta-instructions              |\n",
    "| **Vision input**    | `{\"type\": \"image_url\", \"image_url\": {...}}`      | `{\"type\": \"input_image\", \"image_url\": ...}`                |\n",
    "| **Reasoning / CoT** | Not natively supported                           | `reasoning={\"effort\": ..., \"summary\": ...}` built-in       |\n",
    "| **Response object** | `ChatCompletion` with `choices[]` list           | `Response` with `output[]` list and `output_text` shortcut |\n",
    "| **Streaming**       | `stream=True` yields `ChatCompletionChunk`       | `stream=True` yields server-sent events                    |\n",
    "| **Tool calls**      | Supported via `tools` param                      | Supported via `tools` param (same)                         |\n",
    "| **Model support**   | All chat models                                  | All chat models (newer, recommended going forward)         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd54a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API output: That’s tough to prove—almost every public figure gets\n",
      "flak. If you’re looking for a celebs known more for kindness or generosity,\n",
      "people often point to:  - Keanu Reeves (often cited for humility and generosity)\n",
      "- Dolly Parton (charitable work and warmth) - Tom Hanks (reputation for decency)\n",
      "But “not a jerk” is hard to verify; public personas aren’t the full story.\n"
     ]
    }
   ],
   "source": [
    "input_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Why is Trump a jerk?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Some people are born that way.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Name a celebrity who is not a jerk.\"}\n",
    "]\n",
    "\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"You are a very candid Journalist.\",\n",
    "    input=input_messages, \n",
    "    reasoning={\"effort\": \"minimal\"},  # None, low, medium, high. \n",
    "    text = {\"verbosity\": \"low\"},\n",
    "    max_output_tokens=1000,\n",
    ")\n",
    "\n",
    "pretty_print(\"Responses API output:\", resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861fd92",
   "metadata": {},
   "source": [
    "link to Documentation\n",
    "\n",
    "[Chat Completions](https://developers.openai.com/api/reference/python/resources/chat/subresources/completions/methods/create)\n",
    "\n",
    "[Responses API](https://developers.openai.com/api/reference/python/resources/responses/methods/create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a6876",
   "metadata": {},
   "source": [
    "no temperature and can not force deterministic output, but can control reasoning effort and verbosity of output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760384de",
   "metadata": {},
   "source": [
    "## Key Differences: Parameters in Responses API vs Chat Completions API\n",
    "\n",
    "### `max_tokens` → `max_output_tokens`\n",
    "In the **Responses API**, the parameter to limit output length is **`max_output_tokens`**, NOT `max_tokens`.\n",
    "\n",
    "```python\n",
    "# Chat Completions API\n",
    "client.chat.completions.create(model=\"gpt-5-nano\", messages=..., max_tokens=50)\n",
    "\n",
    "# Responses API\n",
    "client.responses.create(model=\"gpt-5-nano\", input=..., max_output_tokens=50)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `temperature` — Not Supported on Reasoning Models (GPT-5 family)\n",
    "\n",
    "The entire GPT-5 family (`gpt-5`, `gpt-5-mini`, `gpt-5-nano`) are **reasoning models**. They do **NOT** support `temperature` or `top_p`.\n",
    "\n",
    "| Model Family | Type | `temperature` | `top_p` | `max_output_tokens` |\n",
    "|---|---|---|---|---|\n",
    "| **gpt-4o / gpt-5-nano** | Non-reasoning | ✅ Supported | ✅ Supported | ✅ Supported |\n",
    "| **gpt-5 / gpt-5-mini / gpt-5-nano** | Reasoning | ❌ Not supported | ❌ Not supported | ✅ Supported |\n",
    "\n",
    "---\n",
    "\n",
    "### How to Influence Creativity in GPT-5 Reasoning Models\n",
    "\n",
    "Since `temperature` is locked, you control creativity through:\n",
    "\n",
    "**1. `reasoning.effort` parameter** — controls how deeply the model thinks:\n",
    "- `\"low\"` → concise, more deterministic\n",
    "- `\"medium\"` → balanced\n",
    "- `\"high\"` → deeper reasoning, more elaborate and exploratory\n",
    "\n",
    "```python\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning={\"effort\": \"high\", \"summary\": \"auto\"},\n",
    "    input=\"Write a creative poem about Python.\"\n",
    ")\n",
    "```\n",
    "\n",
    "**2. Prompt Engineering** — steer creativity through instructions:\n",
    "```python\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"Be wildly creative. Use unexpected metaphors.\",\n",
    "    input=\"Write a poem about Python.\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Bottom line:** With GPT-5 models, creativity = `reasoning.effort` + prompt wording, not `temperature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dfb132",
   "metadata": {},
   "source": [
    "# How to refer to previous response in the conversation history?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757a616",
   "metadata": {},
   "source": [
    "## Store = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010cb836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: A list comprehension is a compact way to create a new list by applying\n",
      "an expression to each item in an iterable (like a list) and optionally filtering\n",
      "items. It’s a single line alternative to a for-loop that builds a list.  Syntax:\n",
      "- Basic: [expression for item in iterable] - With a filter: [expression for item\n",
      "in iterable if condition]  Examples: - Squares of numbers 0–4: [x*x for x in\n",
      "range(5)]   -> [0, 1, 4, 9, 16]  - Even numbers from a list: [n for n in\n",
      "[1,2,3,4,5] if n % 2 == 0]   -> [2, 4]  - Transforming strings: [s.upper() for s\n",
      "in [\"a\",\"b\",\"c\"]]   -> [\"A\", \"B\", \"C\"]  Benefits: shorter code, often faster,\n",
      "readable once you’re familiar with the syntax. Use when the logic is simple; for\n",
      "complex cases, a regular loop may be clearer.\n"
     ]
    }
   ],
   "source": [
    "# Turn 1\n",
    "message1 = 'What is a list comprehension?'\n",
    "\n",
    "\n",
    "resp1 = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"You are a friendly Python tutor.\",\n",
    "    input=message1,\n",
    "\treasoning={\"effort\": \"minimal\"},   \n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "pretty_print(\"Turn 1:\", resp1.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60d64030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 2: Sure. Here’s a simple example: create a list of squared numbers from 0\n",
      "to 9.  Code: squares = [x*x for x in range(10)] print(squares)  Output: [0, 1,\n",
      "4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# Turn 2 — just pass previous_response_id, no history needed!\n",
    "resp2 = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"Can you give me an example?\",\n",
    "    previous_response_id=resp1.id,  # <-- this is the magic\n",
    "\treasoning={\"effort\": \"minimal\"},   \n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "pretty_print(\"Turn 2:\", resp2.output_text) # This id is stored on open-AI server for 30 days ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_messages = [\n",
    "#    {\"role\": \"user\", \"content\": \"What is a list comprehension?\"},\n",
    "#    {\"role\": \"assistant\", \"content\": resp1.output_text},\n",
    "#    {\"role\": \"user\", \"content\": \"Can you give me an example?\"}\n",
    "#]\n",
    "\n",
    "#resp2_1 = client.responses.create(\n",
    "#    model=\"gpt-5-nano\",\n",
    "#    instructions=\"You are a friendly Python tutor.\",\n",
    "#    input=input_messages,\n",
    "#    reasoning={\"effort\": \"minimal\"},\n",
    "#    text={\"verbosity\": \"low\"}\n",
    "#)\n",
    "#pretty_print(\"Turn 2_1:\", resp2_1.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7543fc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 3: Your first question was: \"What is a list comprehension?\"\n"
     ]
    }
   ],
   "source": [
    "# Turn 3 — chains from turn 2 (which already includes turn 1)\n",
    "resp3 = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    input=\"What was my first question?\",\n",
    "    previous_response_id=resp2.id,\n",
    "\treasoning={\"effort\": \"minimal\"},   \n",
    "    text={\"verbosity\": \"low\"}\n",
    ")\n",
    "pretty_print(\"Turn 3:\", resp3.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8b0f5d",
   "metadata": {},
   "source": [
    "## Store = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "587cd938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 4: A list comprehension is a concise way to create a new list by applying\n",
      "an expression to each item in an iterable (like a list) and optionally filtering\n",
      "items. It’s a compact syntax that combines looping and building the list.  Basic\n",
      "form: - [expression for item in iterable] Optional filter: - [expression for\n",
      "item in iterable if condition]  Examples: - squares of numbers 0–9: [x*x for x\n",
      "in range(10)] - even numbers from 0 to 19: [x for x in range(20) if x % 2 == 0]\n",
      "Benefits: shorter, often faster, clearer once you’re used to the syntax. Use for\n",
      "simple cases; for complex logic, a regular loop may be clearer.\n"
     ]
    }
   ],
   "source": [
    "# Turn 1\n",
    "resp4 = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"You are a friendly Python tutor.\",\n",
    "    input=\"What is a list comprehension?\",\n",
    "\treasoning={\"effort\": \"minimal\"},   \n",
    "    text={\"verbosity\": \"low\"},\n",
    "\tstore=False\n",
    ")\n",
    "pretty_print(\"Turn 4:\", resp4.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b923b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating response: Error code: 400 - {'error': {'message': \"Previous\n",
      "response with id 'resp_059f8d96db81a3bc01699dcb228188819b9f4c76fb0ed8eaaa' not\n",
      "found.\", 'type': 'invalid_request_error', 'param': 'previous_response_id',\n",
      "'code': 'previous_response_not_found'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Turn 5 — chains from turn 4 \n",
    "\n",
    "try:\n",
    "    resp5 = client.responses.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        input=\"What was my first question?\",\n",
    "        previous_response_id=resp4.id,\n",
    "        reasoning={\"effort\": \"minimal\"},   \n",
    "        text={\"verbosity\": \"low\"}\n",
    "    )\n",
    "    pretty_print(\"Turn 5:\", resp5.output_text)\n",
    "except Exception as e:\n",
    "    pretty_print(\"Error creating response:\", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584a563",
   "metadata": {},
   "source": [
    "# Nature of Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e87cb7",
   "metadata": {},
   "source": [
    "## Scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f706d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TURN 1:\n",
      " {\n",
      "  \"severity\": \"critical\",\n",
      "  \"suspected_causes\": [\n",
      "    \"Backend webhook delivery service degradation or outage causing repeated 502 responses\",\n",
      "    \"Recent deployment or configuration change in the webhook delivery path misconfiguring upstream integration\",\n",
      "    \"Network connectivity issues between our delivery infrastructure and customer endpoints (DNS, TLS handshake, firewall blocks)\",\n",
      "    \"Customer endpoints intermittently returning invalid responses during retries (leading to 502 from gateway)\",\n",
      "    \"Gateway/edge load or misconfiguration causing a 502 when handling high retry volume\"\n",
      "  ],\n",
      "  \"next_questions\": [\n",
      "    \"Is this affecting all subscribers or a subset of endpoints on the Pro plan?\",\n",
      "    \"Do you have delivery IDs, timestamps, and the webhook endpoint URLs affected so we can correlate logs?\",\n",
      "    \"Have there been any recent changes to your receiving endpoints (codes, TLS certs, firewall rules)?\",\n",
      "    \"Are the affected endpoints in a specific region or across multiple regions?\",\n",
      "    \"What is the configured retry policy for webhook deliveries (backoff, max retries)?\",\n",
      "    \"Can you share a sample of the 502 responses and any accompanying error messages from our gateway?\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "r1 = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    instructions=\"You are an internal support triage bot. Return only valid JSON with keys severity,suspected_causes,next_questions and do not write any customer-facing text.\",\n",
    "    input=\"A customer reports webhook deliveries started retrying heavily since 10:42 UTC and they see 502 errors from our endpoint on the Pro plan.\"\n",
    ")\n",
    "\n",
    "print(\"TURN 1:\\n\", r1.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69755c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TURN 2:\n",
      " Subject: We’re investigating the 502 errors on your webhook deliveries\n",
      "\n",
      "Hi there,\n",
      "\n",
      "I’m sorry for the disruption. We’re investigating why webhook deliveries on your Pro plan started retrying heavily and showing 502 errors since 10:42 UTC.\n",
      "\n",
      "What we’re checking now:\n",
      "- Our webhook delivery service health and any recent deployments or configuration changes\n",
      "- Network connectivity between our delivery infrastructure and your endpoints (DNS, TLS, firewalls)\n",
      "- Gateway and retry logs to understand the scope and timing of failures\n",
      "- Whether your endpoints are intermittently returning errors during retries\n",
      "\n",
      "To help us resolve this faster, could you please provide two details:\n",
      "1) A few delivery IDs (or timestamps) for the 502 failures, so we can locate them in our logs\n",
      "2) The exact webhook endpoint URLs that were affected (including the region, if applicable)\n",
      "\n",
      "If you have any other details you think might help, feel free to share them.\n",
      "\n",
      "We’ll keep you updated as we learn more. Thank you for your patience.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "[Company/Team]\n"
     ]
    }
   ],
   "source": [
    "r2 = client.responses.create(\n",
    "    model=\"gpt-5-nano\",\n",
    "    previous_response_id=r1.id,\n",
    "    input=\"Now write a customer-facing email reply: apologize, explain what we’re checking, and ask for 2 specific details. Plain English, not JSON.\"\n",
    ")\n",
    "\n",
    "print(\"\\nTURN 2:\\n\", r2.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81f75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3274a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
